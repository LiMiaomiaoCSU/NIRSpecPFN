import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.model_selection import GridSearchCV
from tabpfn import TabPFNRegressor
from sklearn.cross_decomposition import PLSRegression
from sklearn.svm import SVR
from data_processor import DataProcessor
from sklearn.model_selection import KFold

if __name__ == "__main__":
    # 加载数据
    df = pd.read_excel('D:/A/CSU/数据集/3-标样光谱性质数据表/144（完整版）.xlsx')
    # 提取通道数据和目标变量
    spectra = df.iloc[:, :256].values  
    y = df.iloc[:, 257].values.ravel()  
    
    # 初始化数据处理器
    processor = DataProcessor(spectra, y)
    
    # 定义数据划分配置 - 使用字典形式
    split_config = {
        'methods': [
            {'name': 'random', 'params': {'test_size': 0.25, 'random_state': 42}},
            {'name': 'spxy', 'params': {'test_size': 0.25}},
            {'name': 'kennard-stone', 'params': {'test_size': 0.25}}
        ],
        'active_method': 'spxy'  # 指定要使用的数据划分方法
    }
    
    # 根据配置加载并分割数据集
    for method_entry in split_config['methods']:
        method_name = method_entry['name']
        method_params = method_entry['params']
        
        if method_name == split_config['active_method']:
            print(f"使用数据划分方法: {method_name}")
            
            if method_name == 'random':
                processor.load_and_split_data(
                    method='random',
                    test_size=method_params['test_size'],
                    random_state=method_params['random_state']
                )
            elif method_name == 'spxy':
                processor.load_and_split_data(
                    method='spxy',
                    test_size=method_params['test_size']
                )
            elif method_name == 'kennard-stone':
                processor.load_and_split_data(
                    method='kennard-stone',
                    test_size=method_params['test_size']
                )
            break
    
    # 定义训练集样本数量列表
    sample_sizes = [15, 25, 35, 45, 55, 65, 75, 85, 95, 100]
    
    # 定义预处理和特征选择方法
    preprocess_methods = ['detrend']  
    feature_selection_methods = ['uniform_sampling']  
    count = 200  
    
    # 定义模型及其参数网格
    models = {
        'PLSR': {
            'estimator': PLSRegression(),
            'param_grid': {'n_components': [5, 10, 15, 20]}
        },
        'SVR': {
            'estimator': SVR(),
            'param_grid': {'kernel': ['linear', 'rbf'], 'C': [0.1, 1, 10], 'gamma': ['scale', 'auto']}
        },
        'Random Forest': {
            'estimator': RandomForestRegressor(random_state=42),
            'param_grid': {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]}
        },
        'TabPFN': {
            'estimator': TabPFNRegressor(device='cpu', random_state=42),
            'param_grid': {}  
        }
    }
    
    # 创建字典来存储不同训练集样本数量下的结果
    all_results = {sample_size: {'predictions': {model_name: [] for model_name in models.keys()},
                                'best_params': {model_name: [] for model_name in models.keys()},
                                'mse_results': {model_name: [] for model_name in models.keys()},
                                'r2_results': {model_name: [] for model_name in models.keys()}}

                  for sample_size in sample_sizes}
    
    # 使用 k 折交叉验证
    cv = KFold(n_splits=5, shuffle=True, random_state=42)  # 5 折交叉验证
    
    # 遍历不同的训练集样本数量
    for sample_size in sample_sizes:
        # 确保训练集样本数量不超过实际训练集大小
        actual_train_size = min(sample_size, processor.X_cal.shape[0])
        
        # 根据选择的方法选择新的训练样本
        new_X_cal, new_y_cal = processor.select_train_samples(
            method=split_config['active_method'],
            sample_size=sample_size - len(processor.selected_indices)
        )
        
        # 获取当前的训练集和完整的测试集
        current_X_cal = processor.X_cal[processor.selected_indices]
        current_y_cal = processor.y_cal[processor.selected_indices]
        X_test = processor.X_test
        y_test = processor.y_test
        
        # 处理数据
        processed_data = {}
        
        # 应用预处理方法
        processed_X_cal = current_X_cal.copy()
        processed_X_test = X_test.copy()
        
        if 'airPLS' in preprocess_methods:
            processed_X_cal = processor.baseline_correction_airPLS(processed_X_cal, lambda_=100, itermax=15)
            processed_X_test = processor.baseline_correction_airPLS(processed_X_test, lambda_=100, itermax=15)
        
        if 'MSC' in preprocess_methods:
            processed_X_cal = processor.perform_msc(processed_X_cal)
            processed_X_test = processor.perform_msc(processed_X_test)
        
        if 'SNV' in preprocess_methods:
            processed_X_cal = processor.perform_standard_normal_variate(processed_X_cal)
            processed_X_test = processor.perform_standard_normal_variate(processed_X_test)
        
        if 'Savitzky-Golay' in preprocess_methods:
            processed_X_cal = processor.perform_savgol(processed_X_cal)
            processed_X_test = processor.perform_savgol(processed_X_test)

        if 'detrend' in preprocess_methods:
            processed_X_cal = processor.perform_detrend(processed_X_cal)
            processed_X_test = processor.perform_detrend(processed_X_test)

        if 'derivative' in preprocess_methods:
            processed_X_cal = processor.spectral_first_order_derivative(processed_X_cal)
            processed_X_test = processor.spectral_first_order_derivative(processed_X_test)
        
        # 应用特征选择方法
        for method in feature_selection_methods:
            if method == 'uniform_sampling':
                sampled_cal, sampled_test = processor.uniform_sampling(processed_X_cal, processed_X_test, count=count)
                processed_data[f'uniform_sampling_{count}'] = {'cal': sampled_cal, 'test': sampled_test}
            elif method == 'univariant_selection':
                selected_indices = processor.perform_univariant_selection(processed_X_cal, current_y_cal, count=count)
                processed_data['univariant_selection'] = {'cal': processed_X_cal[:, selected_indices], 'test': processed_X_test[:, selected_indices]}
            elif method == 'recursive_elimination':
                selected_indices = processor.perform_recursive_elimination(processed_X_cal, current_y_cal, count=count)
                processed_data['recursive_elimination'] = {'cal': processed_X_cal[:, selected_indices], 'test': processed_X_test[:, selected_indices]}
            elif method == 'pca':
                pca_cal = processor.perform_pca(processed_X_cal, count=count)
                pca_test = processor.perform_pca(processed_X_test, count=count)
                processed_data['pca'] = {'cal': pca_cal, 'test': pca_test}
            elif method == 'uve':
                selected_indices = processor.perform_uve(processed_X_cal, current_y_cal, count=count)
                processed_data['uve'] = {'cal': processed_X_cal[:, selected_indices], 'test': processed_X_test[:, selected_indices]}
        
        print(f"数据处理完成，训练集样本数量: {actual_train_size}")
        print("处理后的数据键:", list(processed_data.keys()))
        
        # 对每个处理后的数据集进行建模和评估
        for method_name, method_data in processed_data.items():
            X_cal, X_test = method_data['cal'], method_data['test']
            y_cal, y_test = current_y_cal, processor.y_test
            
            # 确保数据是2D数组
            if X_cal.ndim == 1:
                X_cal = X_cal.reshape(-1, 1)
            if X_test.ndim == 1:
                X_test = X_test.reshape(-1, 1)
            
            for model_name, model_info in models.items():
                print(f"训练模型: {model_name}...")
                model = model_info['estimator']
                param_grid = model_info['param_grid']
                
                if param_grid:  
                    grid_search = GridSearchCV(model, param_grid, cv=3, scoring='neg_mean_squared_error', verbose=0)
                    grid_search.fit(X_cal, y_cal)
                    best_model = grid_search.best_estimator_
                    best_params = grid_search.best_params_
                else:  
                    best_model = model
                    best_params = 'Default params'
                    best_model.fit(X_cal, y_cal)
                
                # 使用 k 折交叉验证
                cv_scores = []
                for train_index, val_index in cv.split(X_cal):
                    X_train, X_val = X_cal[train_index], X_cal[val_index]
                    y_train, y_val = y_cal[train_index], y_cal[val_index]
                    
                    best_model.fit(X_train, y_train)
                    y_pred = best_model.predict(X_val)
                    
                    mse = mean_squared_error(y_val, y_pred)
                    r2 = r2_score(y_val, y_pred)
                    
                    cv_scores.append({'mse': mse, 'r2': r2})
                
                # 计算交叉验证的平均性能
                avg_mse = np.mean([score['mse'] for score in cv_scores])
                avg_r2 = np.mean([score['r2'] for score in cv_scores])
                
                # 预测
                y_pred = best_model.predict(X_test)
                
                # 评估
                mse = mean_squared_error(y_test, y_pred)
                r2 = r2_score(y_test, y_pred)
                
                # 存储结果
                all_results[sample_size]['predictions'][model_name].append(y_pred)
                all_results[sample_size]['best_params'][model_name].append(best_params)
                all_results[sample_size]['mse_results'][model_name].append(mse)
                all_results[sample_size]['r2_results'][model_name].append(r2)
                
                print(f"训练集样本数量: {actual_train_size}, 特征选择方法: {method_name}, 模型: {model_name}, 最佳参数: {best_params}, 交叉验证 MSE: {avg_mse:.4f}, 交叉验证 R²: {avg_r2:.4f}, 测试集 MSE: {mse:.4f}, 测试集 R²: {r2:.4f}")
    
    # 打印模型的性能结果和最佳参数
    print("\n最终模型性能评估:")
    for sample_size in sample_sizes:
        print(f"\n训练集样本数量: {sample_size}")
        for model_name in models.keys():
            mse = all_results[sample_size]['mse_results'][model_name][0]
            r2 = all_results[sample_size]['r2_results'][model_name][0]
            params = all_results[sample_size]['best_params'][model_name][0]
            print(f"模型: {model_name}, 最佳参数: {params}, MSE: {mse:.4f}, R²: {r2:.4f}")
